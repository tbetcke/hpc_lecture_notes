{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba Cuda in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable Cuda in Numba with conda just execute `conda install cudatoolkit` on the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cuda extension supports almost all Cuda features with the exception of dynamic parallelism and texture memory.  Dynamic parallelism allows to launch compute kernel from within other compute kernels. Texture memory has a caching pattern based on spatial locality. We will not go into detail of these here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding out about Cuda devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first check what kind of Cuda device we have in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "id 0      b'Quadro RTX 3000'                              [SUPPORTED]\n",
      "                      compute capability: 7.5\n",
      "                           pci device id: 0\n",
      "                              pci bus id: 1\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.detect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching a Cuda kernel from Numba is very easy. A kernel is defined by using the `@cuda.jit` decorator as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def an_empty_kernel():\n",
    "    \"\"\"A kernel that doesn't do anything.\"\"\"\n",
    "    # Get my current position in the global grid\n",
    "    [pos_x, pos_y] = cuda.grid(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of the kernel is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numba.cuda.compiler.Dispatcher at 0x7f0dfc812d90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an_empty_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to launch the kernel we need to specify the thread layout. The following commands define a two dimensional thread layout of $16\\times 16$ threads per block and $256\\times 256$ blocks. In total this gives us $16,777,216$ threads. This sounds huge. But GPUs are designed to launch large amounts of threads. The only restriction is that we are allowed to have at most 1024 threads in total (product of all dimensions) within a single thread block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "threadsperblock = (16, 16) # Should be a multiple of 32 if possible.\n",
    "blockspergrid = (256, 256) # Blocks per grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now launch all 16.8 million threads by calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_empty_kernel[blockspergrid, threadsperblock]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside a kernel we can use the following commands to get the position of the thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def another_kernel():\n",
    "    \"\"\"Commands to get thread positions\"\"\"\n",
    "    # Get the thread position in a thread block\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    tz = cuda.threadIdx.z\n",
    "    \n",
    "    # Get the id of the thread block\n",
    "    block_x = cuda.blockIdx.x\n",
    "    block_y = cuda.blockIdx.y\n",
    "    block_z = cuda.blockIdx.z\n",
    "    \n",
    "    # Number of threads per block\n",
    "    dim_x = cuda.blockDim.x\n",
    "    dim_y = cuda.blockDim.y\n",
    "    dim_z = cuda.blockDim.z\n",
    "    \n",
    "    # Global thread position\n",
    "    pos_x = tx + block_x * dim_x\n",
    "    pos_y = ty + block_y * dim_y\n",
    "    pos_z = tz + block_z * dim_z\n",
    "    \n",
    "    # We can also use the grid function to get\n",
    "    # the global position\n",
    "    \n",
    "    (pos_x, pos_y, pos_z) = cuda.grid(3)\n",
    "    # For a 1-or 2-d grid use grid(1) or grid(2)\n",
    "    # to return a scalar or a two tuple.\n",
    "    \n",
    "    \n",
    "threadsperblock = (16, 16, 4) # Should be a multiple of 32 if possible.\n",
    "blockspergrid = (256, 256, 256) # Blocks per grid\n",
    "\n",
    "another_kernel[blockspergrid, threadsperblock]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python features in Numba for Cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba supports in Cuda kernels only a selected set of features that are supported by the Cuda standard. Not allowed are exceptions, context managers, list comprehensions and yield statements. Supported types are `int`, `float`, `complex`, `bool`, `None`, `tuple`. For a complete overview of supported features see [https://numba.pydata.org/numba-doc/dev/cuda/cudapysupported.html#](https://numba.pydata.org/numba-doc/dev/cuda/cudapysupported.html#). Only a small set of Numpy functions are supported. Essentially, everything that does require dynamic memory management will not work due to the restrictions on kernels from the Cuda programming model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple kernels we can rely on Numba copying data to and from the device. For more complex code we need to manually manage buffers on the device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy data to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.arange(10)\n",
    "device_arr = cuda.to_device(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy data from the device back to the host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_arr = device_arr.copy_to_host() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy into an existing array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host_array = np.empty(shape=device_arr.shape, dtype=device_arr.dtype)\n",
    "device_arr.copy_to_host(host_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a new array on the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_array = cuda.device_array((10,), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuda has a number of advanced features that are supported by Numba. Some of them are:\n",
    "\n",
    "* Pinned Memory is a form of memory allocation that allows much faster data transfer than standard buffers.\n",
    "* Streams are a way to run multiple tasks on a GPU concurrently. By default, Cuda executes one command after another on the device. Streams allow us to create several concurrent queues for scheduling tasks onto the device. This allows for example to have a kernel stream that performs computations and a memory stream that does memory transfers, concurrently. One can use events to synchronize between different streams.\n",
    "* Multiple devices are well supported by Numba. There exist helper routines to enumerate and select different devices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full list of features check out the guide at [https://numba.pydata.org/numba-doc/latest/cuda/index.html](https://numba.pydata.org/numba-doc/latest/cuda/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev] *",
   "language": "python",
   "name": "conda-env-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}