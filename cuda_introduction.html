
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>A tour of CUDA &#8212; Techniques of High-Performance Computing - Lecture Notes</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Numba Cuda in Practice" href="numba_cuda.html" />
    <link rel="prev" title="An Introduction to GPU Computing" href="gpu_introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/cpu_logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Techniques of High-Performance Computing - Lecture Notes</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to Techniques of High-Performance Computing
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  High-Performance Computing with Python
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="what_is_hpc.html">
   What is High-Performance Computing?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hpc_languages.html">
   Languages for High-Performance Computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="python_hpc_tools.html">
   Python HPC Tools
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="numpy_and_data_layouts.html">
   Memory layout and Numpy arrays
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="parallel_principles.html">
   Parallel Computing Principles in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="working_with_numba.html">
   Working with Numba
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simd.html">
   SIMD Autovectorization in Numba
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="numexpr.html">
   A Numexpr example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gpu_introduction.html">
   An Introduction to GPU Computing
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   A tour of CUDA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="numba_cuda.html">
   Numba Cuda in Practice
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rbf_evaluation.html">
   GPU accelerated evaluation of particle sums
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Sparse Linear Algebra
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="sparse_linalg_pde.html">
   The need for sparse linear algebra - A PDE example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sparse_data_structures.html">
   Sparse Matrix data structures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sparse_solvers_introduction.html">
   An introduction to sparse linear system solvers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="it_solvers1.html">
   Iterative Solvers 1 - Krylov subspaces, Arnoldi Iteration and the Full Orthogonalisation Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="it_solvers2.html">
   Iterative Solvers 2 - From FOM to GMRES
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="it_solvers3.html">
   Iterative Solvers 3 - The Conjugate Gradient Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="it_solvers4.html">
   Iterative Solvers 4 - Preconditioning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sparse_direct_solvers.html">
   Sparse Direct Solvers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="petsc_for_sparse_systems.html">
   Using petsc4py for sparse linear systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multigrid.html">
   Multigrid Methods
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Time-Dependent Problems
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="simple_time_stepping.html">
   Simple time-stepping
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wave_equation.html">
   Discretising the wave equation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Conclusions
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="further_topics.html">
   Further topics
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Assignments
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="2022-assignment_1.html">
   Assignment 1 - Matrix-matrix multiplication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2022-assignment_2.html">
   Assignment 2 - Solving two 1D problems
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tasks for Monday Practical Classes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="2022-class_1.html">
   Class 1 (Monday 10 October)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2022-class_2.html">
   Class 2 (Monday 17 October)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2022-class_3.html">
   Class 3 (Monday 24 October)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2022-class_4.html">
   Class 4 (Monday 31 October)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Additional notes from 2022
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="2022_matrices_and_simultaneous_equations.html">
   Matrices and simultaneous equations
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/cuda_introduction.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cuda-device-model">
   CUDA Device Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#threads-cuda-cores-warps-and-streaming-multiprocessors">
     Threads, Cuda Cores, Warps and Streaming Multiprocessors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#numbering-of-threads">
     Numbering of threads
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#memory-hierarchy">
     Memory Hierarchy
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#an-example">
   An example
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>A tour of CUDA</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cuda-device-model">
   CUDA Device Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#threads-cuda-cores-warps-and-streaming-multiprocessors">
     Threads, Cuda Cores, Warps and Streaming Multiprocessors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#numbering-of-threads">
     Numbering of threads
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#memory-hierarchy">
     Memory Hierarchy
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#an-example">
   An example
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="a-tour-of-cuda">
<h1>A tour of CUDA<a class="headerlink" href="#a-tour-of-cuda" title="Permalink to this headline">#</a></h1>
<p>In this chapter we will dive into CUDA, the standard GPU development model for Nvidia devices. To understand the basics of CUDA we first need to understand how GPU devices are organised.</p>
<section id="cuda-device-model">
<h2>CUDA Device Model<a class="headerlink" href="#cuda-device-model" title="Permalink to this headline">#</a></h2>
<p>At the most basic level, GPU accelerators are massively parallel compute devices that can run a huge number of threads concurrently. Compute devices have global memory, shared memory and local memory for threads. Moreover, threads are grouped into thread blocks that allow shared memory access. We will discuss all these points in more detail below.</p>
<section id="threads-cuda-cores-warps-and-streaming-multiprocessors">
<h3>Threads, Cuda Cores, Warps and Streaming Multiprocessors<a class="headerlink" href="#threads-cuda-cores-warps-and-streaming-multiprocessors" title="Permalink to this headline">#</a></h3>
<p>A GPU device is organised into a number of Streaming Multiprocessors (SM). Each SM is responsible for scheduling and executing a number of thread blocks. Below we show the design of a SM for the Nvidia A100 Architecture (see <a class="reference external" href="https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/">https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/</a>).</p>
<p><img alt="SM Architecture" src="_images/a100_sm.png" /></p>
<p>Each SM in the A100 architecture consists of integer cores, floating point cores and tensor cores. Tensor cores are relatively new and optimised for mixed precision multiply/add operations for deep learning. The SM is responsible for scheduling threads onto the different compute cores. For the developer the lowest logical entity is a thread. Threads are organised by thread blocks in CUDA. A thread block is a group of thread that is allowed to access fast shared memory together. In terms of implementation thread blocks are divided into Warps, where as each Warp contains 32 threads. Within a Warp all threads must follow the same execution path, which has implications for branch statements that we will discuss later. A Warp is roughly comparable to a SIMD vector register in CPU architectures.</p>
<p>The scheduling into Warps is important for the organisation of thread blocks. Ideally, these are multiples of 32. If a thread block is not a multiple of 32 Cores may be underutilised. Consider a thread block of 48 threads. This will take up two Warps as we have 32 + 16 threads. Hence, the second Warp will not be fully utilised.</p>
</section>
<section id="numbering-of-threads">
<h3>Numbering of threads<a class="headerlink" href="#numbering-of-threads" title="Permalink to this headline">#</a></h3>
<p>The numbering of a thread is shown in the following Figure (see <a class="reference external" href="https://developer.nvidia.com/blog/even-easier-introduction-cuda/">https://developer.nvidia.com/blog/even-easier-introduction-cuda/</a>).</p>
<p><img alt="Thread Numbering" src="_images/thread_numbering.png" /></p>
<p>As mentioned above, threads are organised in thread blocks. All thread blocks together form a thread grid. The thread grid does not need to be one-dimensional. It can also be two or three dimensional. This is convenient if the computational domain is better represented in two or three dimensions. The figure demonstrates how for one dimension the global thread number of a thread block is computed.</p>
</section>
<section id="memory-hierarchy">
<h3>Memory Hierarchy<a class="headerlink" href="#memory-hierarchy" title="Permalink to this headline">#</a></h3>
<p>CUDA knows three types of memory</p>
<ul class="simple">
<li><p>The <strong>global memory</strong> is a block of memory accessible by all threads in a device. This is the largest chunk of memory and the place where we create GPU buffers to store our input to computations and output results. While a GPU typically has a few gigabytes of global memory, access to it is relatively slow from the individual threads.</p></li>
<li><p>All threads within a given block have access to local <strong>shared memory</strong>. This shared memory is fast and available within the lifetime of the thread block. Together with local synchronisation it can be efficiently used to process workload within a given thread block without having to write back and forth to the global memory.</p></li>
<li><p>Each thread has its own <strong>private memory</strong>. This is very fast and used to store local intermediate results that are only needed in the current thread.</p></li>
</ul>
</section>
</section>
<section id="an-example">
<h2>An example<a class="headerlink" href="#an-example" title="Permalink to this headline">#</a></h2>
<p>The following example from the <a class="reference external" href="https://numba.pydata.org/numba-doc/dev/cuda/examples.html#matrix-multiplication">official Numba documentation</a> uses all of the above mentioned principles. It is an implementation of a matrix-matrix product that makes use of shared memory for block-wise multiplication.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">cuda</span><span class="p">,</span> <span class="n">float32</span>

<span class="c1"># Controls threads per block and shared memory usage.</span>
<span class="c1"># The computation will be done on blocks of TPBxTPB elements.</span>
<span class="n">TPB</span> <span class="o">=</span> <span class="mi">16</span>

<span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">fast_matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">):</span>
    <span class="c1"># Define an array in the shared memory</span>
    <span class="c1"># The size and type of the arrays must be known at compile time</span>
    <span class="n">sA</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">TPB</span><span class="p">,</span> <span class="n">TPB</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">sB</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">TPB</span><span class="p">,</span> <span class="n">TPB</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">tx</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="n">ty</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">y</span>
    <span class="n">bpg</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">gridDim</span><span class="o">.</span><span class="n">x</span>    <span class="c1"># blocks per grid</span>

    <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">y</span> <span class="o">&gt;=</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="c1"># Quit if (x, y) is outside of valid C boundary</span>
        <span class="k">return</span>

    <span class="c1"># Each thread computes one element in the result matrix.</span>
    <span class="c1"># The dot product is chunked into dot products of TPB-long vectors.</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bpg</span><span class="p">):</span>
        <span class="c1"># Preload data into shared memory</span>
        <span class="n">sA</span><span class="p">[</span><span class="n">tx</span><span class="p">,</span> <span class="n">ty</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">ty</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">TPB</span><span class="p">]</span>
        <span class="n">sB</span><span class="p">[</span><span class="n">tx</span><span class="p">,</span> <span class="n">ty</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">tx</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">TPB</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span>

        <span class="c1"># Wait until all threads finish preloading</span>
        <span class="n">cuda</span><span class="o">.</span><span class="n">syncthreads</span><span class="p">()</span>

        <span class="c1"># Computes partial product on the shared memory</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">TPB</span><span class="p">):</span>
            <span class="n">tmp</span> <span class="o">+=</span> <span class="n">sA</span><span class="p">[</span><span class="n">tx</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">sB</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">ty</span><span class="p">]</span>

        <span class="c1"># Wait until all threads finish computing</span>
        <span class="n">cuda</span><span class="o">.</span><span class="n">syncthreads</span><span class="p">()</span>

    <span class="n">C</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp</span>
</pre></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="gpu_introduction.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">An Introduction to GPU Computing</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="numba_cuda.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Numba Cuda in Practice</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Timo Betcke & Matthew Scroggs<br/>
  
      &copy; Copyright 2020-22.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>