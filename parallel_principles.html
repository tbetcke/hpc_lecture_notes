
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Parallel Computing Principles in Python &#8212; Techniques of High-Performance Computing - Lecture Notes</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Working with Numba" href="working_with_numba.html" />
    <link rel="prev" title="Memory layout and Numpy arrays" href="numpy_and_data_layouts.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/cpu_logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Techniques of High-Performance Computing - Lecture Notes</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to Techniques of High-Performance Computing
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  High-Performance Computing with Python
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="what_is_hpc.html">
   What is High-Performance Computing?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hpc_languages.html">
   Languages for High-Performance Computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="python_hpc_tools.html">
   Python HPC Tools
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="numpy_and_data_layouts.html">
   Memory layout and Numpy arrays
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Parallel Computing Principles in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="working_with_numba.html">
   Working with Numba
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simd.html">
   SIMD Autovectorization in Numba
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="numexpr.html">
   A Numexpr example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gpu_introduction.html">
   An Introduction to GPU Computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cuda_introduction.html">
   A tour of CUDA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="numba_cuda.html">
   Numba Cuda in Practice
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rbf_evaluation.html">
   GPU accelerated evaluation of particle sums
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Sparse Linear Algebra
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="sparse_linalg_pde.html">
   The need for sparse linear algebra - A PDE example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sparse_data_structures.html">
   Sparse Matrix data structures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sparse_solvers_introduction.html">
   An introduction to sparse linear system solvers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="it_solvers1.html">
   Iterative Solvers 1 - Krylov subspaces, Arnoldi Iteration and the Full Orthogonalisation Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="it_solvers2.html">
   Iterative Solvers 2 - From FOM to GMRES
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="it_solvers3.html">
   Iterative Solvers 3 - The Conjugate Gradient Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="it_solvers4.html">
   Iterative Solvers 4 - Preconditioning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sparse_direct_solvers.html">
   Sparse Direct Solvers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="petsc_for_sparse_systems.html">
   Using petsc4py for sparse linear systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multigrid.html">
   Multigrid Methods
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Time-Dependent Problems
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="simple_time_stepping.html">
   Simple time-stepping
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wave_equation.html">
   Discretising the wave equation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Conclusions
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="further_topics.html">
   Further topics
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Assignments
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="2022-assignment_1.html">
   Assignment 1 - Matrix-matrix multiplication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2022-assignment_2.html">
   Assignment 2 - Solving two 1D problems
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tasks for Monday Practical Classes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="2022-class_1.html">
   Class 1 (Monday 10 October)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2022-class_2.html">
   Class 2 (Monday 17 October)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2022-class_3.html">
   Class 3 (Monday 24 October)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Assignments from past years
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="2021-assignment_1.html">
   Assignment 1 - Matrix multiplication in Numba
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2021-assignment_2.html">
   Assignment 2 - GPU Accelerated solution of Poisson problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2021-assignment_3.html">
   Assignment 3 - Sparse matrix formats on GPUs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2021-assignment_4.html">
   Assignment 4 - Time-dependent problems
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/parallel_principles.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simd-acceleration">
   SIMD Acceleration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multithreading-for-parallel-loop-execution">
   Multithreading for parallel loop execution.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#get-to-know-the-gil">
     Get to know the GIL
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#numba-parallel-threading-without-gil">
     Numba - Parallel threading without GIL
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#an-alternative-solution-process-based-parallel-processing">
     An alternative solution - Process based parallel processing
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Parallel Computing Principles in Python</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simd-acceleration">
   SIMD Acceleration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multithreading-for-parallel-loop-execution">
   Multithreading for parallel loop execution.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#get-to-know-the-gil">
     Get to know the GIL
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#numba-parallel-threading-without-gil">
     Numba - Parallel threading without GIL
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#an-alternative-solution-process-based-parallel-processing">
     An alternative solution - Process based parallel processing
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="parallel-computing-principles-in-python">
<h1>Parallel Computing Principles in Python<a class="headerlink" href="#parallel-computing-principles-in-python" title="Permalink to this headline">#</a></h1>
<p>Modern computers are highly parallel systems. Each CPU consists of multiple CPU cores, and within each CPU cores there are vector units that allow the parallel execution of certain operations. In addition, we have GPU accelerators that are highly parallel devices themselves. If we move to larger compute clusters then there is also a level of parallelism between the individual hardware nodes.</p>
<p>In this chapter we will discuss various layers of parallel execution. We will then demonstrate a number of Python tools that help us with parallel execution.</p>
<p>As a simple example we consider the following simple code-block.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float64&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</pre></div>
</div>
<p>We note that instead of the for-loop we could have just written</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
<p>to let Numpy handle the addition efficiently. But the purpose of this section is to drill a bit deeper and discuss how such a for-loop can be efficiently executed on modern hardware.</p>
<section id="simd-acceleration">
<h2>SIMD Acceleration<a class="headerlink" href="#simd-acceleration" title="Permalink to this headline">#</a></h2>
<p>Almost all modern CPUs support SIMD (Single-Instruction-Multiple-Data) operations using vector registers. The idea is that a CPU core has internal registers that allow the execution of a command on several arguments within a single CPU cycle.We consider the figure below. Let us assume we have a SIMD register <span class="math notranslate nohighlight">\(a\)</span> and a SIMD register <span class="math notranslate nohighlight">\(b\)</span> in the CPU, each of them holding four pieces of data. Then the addition of the four pieces of data can be done all at once within a single CPU cycle.</p>
<p><img alt="SIMD Addition" src="_images/simd_addition.png" /></p>
<p>In principle this allows a factor four speed-up. Most modern CPUs from Intel and AMD support AVX2, a set of CPU instructions that allow to operate on vector registers up to 256 bits in length. This is enough space for four double precision numbers or eight single precision numbers. Not only addition is supported, but a number of operations, including sqrt and multiplication. The most recent standard is AVX-512, which allows simultaneous execution of certain types of 8 double precision or 16 single precision operations. However, this does not usually result in a speed-up of a factor of 8 or 16. CPUs have to reduce their clock speed significantly to execute AVX-512 operations and their invocation is also costly. The outspoken developer of Linux, Linus Torvalds, recently stated that he wishes that <a class="reference external" href="https://www.extremetech.com/computing/312673-linus-torvalds-i-hope-avx512-dies-a-painful-death">“AVX-512 dies a painful death”</a>. The reality is more balanced. For certain HPC and machine learning applications AVX-512 can bring very good speed-ups. Nevertheless, AMD has chosen not to implement AVX-512 in their CPUs and relies on highly optimised AVX2 instructions.</p>
<p>SIMD instructions are a very low-level tool, which we cannot use directly in Python. However, several libraries provide functionality that can take advantage of SIMD instructions, in particular:</p>
<ul class="simple">
<li><p>Numpy benefits from SIMD if the underlying BLAS library uses SIMD instructions.</p></li>
<li><p>The Numexpr library allows the generation of complex transformation over arrays
that will be translated to fast SIMD instructions under certain conditions.</p></li>
<li><p>The Numba Just-In-Time compiler for Python code can auto-compile certain for-loops into accelerated SIMD code.</p></li>
</ul>
<p>We will discuss each of these libraries in more detail later. While there is no direct SIMD control from Python it is important to be aware of SIMD and to use libraries and constructs that guarantee SIMD execution.</p>
</section>
<section id="multithreading-for-parallel-loop-execution">
<h2>Multithreading for parallel loop execution.<a class="headerlink" href="#multithreading-for-parallel-loop-execution" title="Permalink to this headline">#</a></h2>
<p>SIMD is a very low-level acceleration within a single CPU core. In order to execute code over several cores we need to use a different technique. In order to understand this we first have to clarify what is meant by a process and what is meant by a thread.</p>
<p>A <strong>process</strong> within a computer is a self-contained unit of code and associated memory that performs a certain task. Many programs consist of a single process. But some programs use multiple process such as Google Chrome, which has a process for each oben tab. Processes are strictly separated from each other via the operating system, which schedules the execution of processes. A process is not allowed to directly access data from other processes unless through mechanisms provided by the operating system. Moreover, the operating system decides how processes are scheduled onto CPU cores. If you open a task manager, no matter whether Windows, Linux or Mac, you can see dozens or sometimes even hundreds of processes running at the same time.</p>
<p>A <strong>thread</strong> is an execution stream within a process. All threads within a process share the memory provided by the process and are freely able to read and manipulate each others data. Performant applications are highly multithreaded to take advantage of the existing CPU cores in a computer.</p>
<p>Let us return to the for-loop above. All loop iterations are completely independent of each other. We could execute them in any order, or indeed in parallel. The question is how we can achieve this?</p>
<p>Python provides a threading library that allows the creation of multiple threads that executes within a Python program. It is a bit of code overhead. But we could represent a multithreaded for-loop execution in the following way:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">multiprocessing</span>

<span class="k">def</span> <span class="nf">worker</span><span class="p">(</span><span class="n">arr1</span><span class="p">,</span> <span class="n">arr2</span><span class="p">,</span> <span class="n">arr3</span><span class="p">,</span> <span class="n">chunk</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The thread worker.&quot;&quot;&quot;</span>

    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">chunk</span><span class="p">:</span>
        <span class="n">arr3</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr1</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">+</span> <span class="n">arr2</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

<span class="n">nthreads</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float64&#39;</span><span class="p">)</span>

<span class="n">chunks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">nthreads</span><span class="p">)</span>

<span class="n">all_threads</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">:</span>
    <span class="n">thread</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">worker</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">chunk</span><span class="p">))</span>
    <span class="n">all_threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">thread</span><span class="p">)</span>
    <span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="k">for</span> <span class="n">thread</span> <span class="ow">in</span> <span class="n">all_threads</span><span class="p">:</span>
    <span class="n">thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</pre></div>
</div>
<p>How does this code work? We define a <code class="docutils literal notranslate"><span class="pre">worker</span></code> function that takes as three arguments the arrays that we want to sum and the result array. The parameter <code class="docutils literal notranslate"><span class="pre">chunk</span></code> is an integer array of indices that specifies the indices of the elements that we want to sum.</p>
<p>The variable <code class="docutils literal notranslate"><span class="pre">nthreads</span></code> gets the CPU count. We always generate as many threads as there are CPUs in the system.</p>
<p>The variable <code class="docutils literal notranslate"><span class="pre">chunks</span></code> is a list of arrays that splits up the whole index range into chunks of approximate size <code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">/</span> <span class="pre">nthreads</span></code>.</p>
<p>For each index chunk we now generate a new thread by specifying the <code class="docutils literal notranslate"><span class="pre">worker</span></code> function as to be executed in the thread and as parameters the arrays <code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code>, and <code class="docutils literal notranslate"><span class="pre">c</span></code> and the current index set <code class="docutils literal notranslate"><span class="pre">chunk</span></code>. The thread is then started. After calling <code class="docutils literal notranslate"><span class="pre">start</span></code> the next for-loop iteration starts without the thread having to be finished already. The threads run independently of each other.</p>
<p>After all threads are started we are waiting for the thread execution to finish by calling the <code class="docutils literal notranslate"><span class="pre">join</span></code> function for each thread. The <code class="docutils literal notranslate"><span class="pre">join</span></code> function blocks execution until the work inside the thread has concluded.</p>
<p>This way of specifying a thread is very typical and threads in most programming languages follow a similar pattern.</p>
<section id="get-to-know-the-gil">
<h3>Get to know the GIL<a class="headerlink" href="#get-to-know-the-gil" title="Permalink to this headline">#</a></h3>
<p>So far so good. There is only one problem. This code is actually not executing in parallel! The reason is a unique feature of Python, called the GIL (Global Interpreter Lock). When Python was first developed it was decided that only one thread at a time would be able to call into the Python interpreter. There are very good technical performance reason for this choice. But the consequence is that Python threads are not really executing in parallel since when one thread is calling into the interpreter to execute a Python command the other threads have to wait.</p>
<p>So why does threading exist at all in Python? Python threads make sense for I/O bound applications. Consider for example a webserver that is waiting for incoming network connections. You can have one thread waiting for the network while another thread is doing actual work. As long as a thread is not using the GIL another thread can use the interpreter.</p>
</section>
<section id="numba-parallel-threading-without-gil">
<h3>Numba - Parallel threading without GIL<a class="headerlink" href="#numba-parallel-threading-without-gil" title="Permalink to this headline">#</a></h3>
<p>Numba is a library for the Just-In-Time compilation of Python code into low-level machine code that does not require the Python interpreter. We will dive into Numba in a separate session. The beauty about Numba is that since Numba compiled functions do not require the Python interpreter, they execute without having to call into the GIL. This allows to create parallel executing threads independent of the Python interpreter, delivering optimal performance on multicore CPUs. A parallel version of the vector addition in Numba is given below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">numba</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float64&#39;</span><span class="p">)</span>

<span class="nd">@numba</span><span class="o">.</span><span class="n">njit</span><span class="p">(</span><span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">numba_fun</span><span class="p">(</span><span class="n">arr1</span><span class="p">,</span> <span class="n">arr2</span><span class="p">,</span> <span class="n">arr3</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The thread worker.&quot;&quot;&quot;</span>

    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">numba</span><span class="o">.</span><span class="n">prange</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">arr3</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr1</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">+</span> <span class="n">arr2</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

<span class="n">numba_fun</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
<p>In the above example we tell Numba to just-in-time compile the function <code class="docutils literal notranslate"><span class="pre">numba_fun</span></code>. The function <code class="docutils literal notranslate"><span class="pre">prange</span></code> tells Numba that the corresponding for-loop can be parallelised. Numba automatically splits this for-loop into threads that work independently. Since Numba compiles the function into direct machine code that does not require the Python interpreter, the GIL does not interfere.</p>
</section>
<section id="an-alternative-solution-process-based-parallel-processing">
<h3>An alternative solution - Process based parallel processing<a class="headerlink" href="#an-alternative-solution-process-based-parallel-processing" title="Permalink to this headline">#</a></h3>
<p>Python has an alternative solution for parallel execution. We discussed above that threading in Python is limited by the GIL. The solution is process based parallelisation. Instead of multiple threads we use multiple Python processes, each with its own GIL and memory space. The <code class="docutils literal notranslate"><span class="pre">multiprocessing</span></code> module in Python makes dealing with process based parallelisation easy. Below you find the above threading example, but using only the multiprocessing module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">multiprocessing</span>
<span class="kn">import</span> <span class="nn">ctypes</span>

<span class="k">def</span> <span class="nf">worker</span><span class="p">(</span><span class="n">arr1</span><span class="p">,</span> <span class="n">arr2</span><span class="p">,</span> <span class="n">arr3</span><span class="p">,</span> <span class="n">chunk</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The thread worker.&quot;&quot;&quot;</span>

    <span class="c1"># Create Numpy arrays from the</span>
    <span class="c1"># shared multiprocessing arrays</span>

    <span class="n">arr1_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">arr1</span><span class="o">.</span><span class="n">get_obj</span><span class="p">())</span>
    <span class="n">arr2_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">arr2</span><span class="o">.</span><span class="n">get_obj</span><span class="p">())</span>
    <span class="n">arr3_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">arr3</span><span class="o">.</span><span class="n">get_obj</span><span class="p">())</span>

    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">chunk</span><span class="p">:</span>
        <span class="n">arr3_np</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr1_np</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">+</span> <span class="n">arr2_np</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

<span class="n">nprocesses</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">1000000</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Array</span><span class="p">(</span><span class="n">ctypes</span><span class="o">.</span><span class="n">c_double</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Array</span><span class="p">(</span><span class="n">ctypes</span><span class="o">.</span><span class="n">c_double</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Array</span><span class="p">(</span><span class="n">ctypes</span><span class="o">.</span><span class="n">c_double</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>


<span class="n">a</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">b</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">chunks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">nprocesses</span><span class="p">)</span>

<span class="n">all_processes</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">:</span>
    <span class="n">process</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">worker</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">chunk</span><span class="p">))</span>
    <span class="n">all_processes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">process</span><span class="p">)</span>
    <span class="n">process</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="k">for</span> <span class="n">process</span> <span class="ow">in</span> <span class="n">all_processes</span><span class="p">:</span>
    <span class="n">process</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</pre></div>
</div>
<p>This example is very similar to the threading example. The main difference is the variable initialisation. Processes do not share the same memory. The multiprocessing module can copy over variables on intialisation automatically to the different processes. However, this is inefficient for large arrays, and we cannot easily write into a large array. The solution is to create shared arrays. These are special structures that can be accessed from all processes. The <code class="docutils literal notranslate"><span class="pre">multiprocessing.Array</span></code> type serves this purpose. It is very low-level. However, we can create a view of them as a Numpy array. This is done through the <code class="docutils literal notranslate"><span class="pre">np.frombuffer</span></code> command, which creates a Numpy type array based on the shared memory.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="numpy_and_data_layouts.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Memory layout and Numpy arrays</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="working_with_numba.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Working with Numba</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Timo Betcke & Matthew Scroggs<br/>
  
      &copy; Copyright 2020-22.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>